{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mido\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to list folders and subfolders\n",
    "def list_files_and_subfolders(folder_path):\n",
    "    file_names = []\n",
    "    subfolder_names = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        # Print subfolders\n",
    "        for directory in dirs:\n",
    "          subfolder = os.path.join(root, directory)\n",
    "          subfolder_names.append(subfolder)\n",
    "        # Print files\n",
    "        for file in files:\n",
    "\n",
    "            name = os.path.join(root, file)\n",
    "            #print (name)\n",
    "            file_names.append(name)\n",
    "    return subfolder_names, file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GM_dict = {\n",
    "    # key is midi note number\n",
    "    # values are:\n",
    "    # [0] name (as string)\n",
    "    # [1] name category low mid or high (as string)\n",
    "    # [2] substiture midi number for simplified MIDI (all instruments)\n",
    "    # [3] name of instrument for 8 note conversion (as string)\n",
    "    # [4] number of instrument for 8 note conversion\n",
    "    # [5] substiture midi number for conversion to 8 note\n",
    "    # [6] substiture midi number for conversion to 16 note\n",
    "    # [7] substiture midi number for conversion to 3 note\n",
    "    # if we are going to remap just use GM_dict[msg.note][X]\n",
    "    22: [\"Closed Hi-Hat edge\", \"high\", 42, \"CH\", 3, 42, 42, 42],\n",
    "    26: [\"Open Hi-Hat edge\", \"high\", 46, \"OH\", 4, 46, 46, 42],\n",
    "    35: [\"Acoustic Bass Drum\", \"low\", 36, \"K\", 1, 36, 36, 36],\n",
    "    36: [\"Bass Drum 1\", \"low\", 36, \"K\", 1, 36, 36, 36],\n",
    "    37: [\"Side Stick\", \"mid\", 37, \"RS\", 6, 37, 37, 38],\n",
    "    38: [\"Acoustic Snare\", \"mid\", 38, \"SN\", 2, 38, 38, 38],\n",
    "    39: [\"Hand Clap\", \"mid\", 39, \"CP\", 5, 39, 39, 38],\n",
    "    40: [\"Electric Snare\", \"mid\", 38, \"SN\", 2, 38, 38, 38],\n",
    "    41: [\"Low Floor Tom\", \"low\", 45, \"LT\", 7, 45, 45, 36],\n",
    "    42: [\"Closed Hi Hat\", \"high\", 42, \"CH\", 3, 42, 42, 42],\n",
    "    43: [\"High Floor Tom\", \"mid\", 45, \"HT\", 8, 45, 45, 38],\n",
    "    44: [\"Pedal Hi-Hat\", \"high\", 46, \"OH\", 4, 46, 46, 42],\n",
    "    45: [\"Low Tom\", \"low\", 45, \"LT\", 7, 45, 45, 36],\n",
    "    46: [\"Open Hi-Hat\", \"high\", 46, \"OH\", 4, 46, 46, 42],\n",
    "    47: [\"Low-Mid Tom\", \"low\", 47, \"MT\", 7, 45, 47, 36],\n",
    "    48: [\"Hi-Mid Tom\", \"mid\", 47, \"MT\", 7, 50, 50, 38],\n",
    "    49: [\"Crash Cymbal 1\", \"high\", 49, \"CC\", 4, 46, 42, 42],\n",
    "    50: [\"High Tom\", \"mid\", 50, \"HT\", 8, 50, 50, 38],\n",
    "    51: [\"Ride Cymbal 1\", \"high\", 51, \"RC\", -1, 42, 51, 42],\n",
    "    52: [\"Chinese Cymbal\", \"high\", 52, \"\", -1, 46, 51, 42],\n",
    "    53: [\"Ride Bell\", \"high\", 53, \"\", -1, 42, 51, 42],\n",
    "    54: [\"Tambourine\", \"high\", 54, \"\", -1, 42, 69, 42],\n",
    "    55: [\"Splash Cymbal\", \"high\", 55, \"OH\", 4, 46, 42, 42],\n",
    "    56: [\"Cowbell\", \"high\", 56, \"CB\", -1, 37, 56, 42],\n",
    "    57: [\"Crash Cymbal 2\", \"high\", 57, \"CC\", 4, 46, 42, 42],\n",
    "    58: [\"Vibraslap\", \"mid\", 58, \"VS\", 6, 37, 37, 42],\n",
    "    59: [\"Ride Cymbal 2\", \"high\", 59, \"RC\", 3, 42, 51, 42],\n",
    "    60: [\"Hi Bongo\", \"high\", 60, \"LB\", 8, 45, 63, 42],\n",
    "    61: [\"Low Bongo\", \"mid\", 61, \"HB\", 7, 45, 64, 38],\n",
    "    62: [\"Mute Hi Conga\", \"mid\", 62, \"MC\", 8, 50, 62, 38],\n",
    "    63: [\"Open Hi Conga\", \"high\", 63, \"HC\", 8, 50, 63, 42],\n",
    "    64: [\"Low Conga\", \"low\", 64, \"LC\", 7, 45, 64, 36],\n",
    "    65: [\"High Timbale\", \"mid\", 65, \"\", 8, 45, 63, 38],\n",
    "    66: [\"Low Timbale\", \"low\", 66, \"\", 7, 45, 64, 36],\n",
    "    67: [\"High Agogo\", \"high\", 67, \"\", -1, 37, 56, 42],\n",
    "    68: [\"Low Agogo\", \"mid\", 68, \"\", -1, 37, 56, 38],\n",
    "    69: [\"Cabasa\", \"high\", 69, \"MA\", -1, 42, 69, 42],\n",
    "    70: [\"Maracas\", \"high\", 69, \"MA\", -1, 42, 69, 42],\n",
    "    71: [\"Short Whistle\", \"high\", 71, \"\", -1, 37, 56, 42],\n",
    "    72: [\"Long Whistle\", \"high\", 72, \"\", -1, 37, 56, 42],\n",
    "    73: [\"Short Guiro\", \"high\", 73, \"\", -1, 42, 42, 42],\n",
    "    74: [\"Long Guiro\", \"high\", 74, \"\", -1, 46, 46, 42],\n",
    "    75: [\"Claves\", \"high\", 75, \"\", -1, 37, 75, 42],\n",
    "    76: [\"Hi Wood Block\", \"high\", 76, \"\", 8, 50, 63, 42],\n",
    "    77: [\"Low Wood Block\", \"mid\", 77, \"\", 7, 45, 64, 38],\n",
    "    78: [\"Mute Cuica\", \"high\", 78, \"\", -1, 50, 62, 42],\n",
    "    79: [\"Open Cuica\", \"high\", 79, \"\", -1, 45, 63, 42],\n",
    "    80: [\"Mute Triangle\", \"high\", 80, \"\", -1, 37, 75, 42],\n",
    "    81: [\"Open Triangle\", \"high\", 81, \"\", -1, 37, 75, 42],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midifile2hv_list(file_name, mapping):\n",
    "    '''\n",
    "    pattern name must include .mid\n",
    "    get a MIDI file and convert it to an hv_list (a list of note numbers and velocity)\n",
    "    use the \"mapping\" variable to define the type of instrument mapping\n",
    "    that will be used in the hv_list \"all\", \"16\", \"8\", \"3\"\n",
    "    '''\n",
    "    pattern=[]\n",
    "    mid=mido.MidiFile(file_name) #create a mido file instance\n",
    "    sixteenth= mid.ticks_per_beat/4 #find the length of a sixteenth note\n",
    "    #print (\"sixteenth\", sixteenth)\n",
    "\n",
    "    # time: inside a track, it is delta time in ticks (integrer).\n",
    "    # A delta time is how long to wait before the next message.\n",
    "    acc=0 #use this to keep track of time\n",
    "\n",
    "    # depending on the instruments variable select a notemapping\n",
    "    if mapping==\"allinstruments\":\n",
    "        column=2\n",
    "    elif mapping==\"16instruments\":\n",
    "        column=6\n",
    "    elif mapping==\"8instruments\":\n",
    "        column=5\n",
    "    elif mapping==\"3instruments\":\n",
    "        column=7\n",
    "    else: column = 2 # if no mapping is selected use \"allinstrument\" mapping\n",
    "\n",
    "    for i, track in enumerate(mid.tracks):\n",
    "        for msg in track: #process all messages\n",
    "            acc += msg.time # accumulate time of any message type\n",
    "            if msg.type == \"note_on\" and msg.velocity != 0: # skip velocity 0 format of note off\n",
    "                if msg.note in list(GM_dict.keys()):\n",
    "                  midinote = GM_dict[msg.note][column] #remap msg.note by demand\n",
    "                  rounded_step = int((acc/sixteenth)+0.45)\n",
    "                  midivelocity = msg.velocity/127 # normalize upfront\n",
    "                  pattern.append((int(acc/sixteenth), midinote, midivelocity)) # step, note, velocity\n",
    "\n",
    "        if len(pattern)>0: #just proceed if analyzed pattern has at least one onset\n",
    "\n",
    "            #round the pattern to the next multiple of 16\n",
    "            if (rounded_step/16) - (rounded_step//16) != 0:\n",
    "                pattern_len_in_steps = (rounded_step//16)*16 + 16\n",
    "            else:\n",
    "                pattern_len_in_steps = (rounded_step//16)*16\n",
    "\n",
    "            #create an empty list of lists the size of the pattern\n",
    "            output_pattern=[[]]*pattern_len_in_steps\n",
    "            # group the instruments and their velocity that played at a specific step\n",
    "            i = 0\n",
    "            for step in range(len(output_pattern)):\n",
    "                output_pattern.append([(x[1],x[2]) for x in pattern if x[0]==step])\n",
    "                #make sure no notes are repeated and events are sorted\n",
    "                output_pattern[step] = list(set(output_pattern[step]))\n",
    "                output_pattern[step].sort()\n",
    "\n",
    "    ##################################\n",
    "    # split the pattern every 16 steps\n",
    "    ##################################\n",
    "    hv_lists_split=[]\n",
    "    for x in range(len(output_pattern)//16):\n",
    "        patt_fragment = output_pattern[x*16:(x*16)+16]\n",
    "        patt_density = sum([1 for x in patt_fragment if x!=[]])\n",
    "\n",
    "        #############################################################\n",
    "        # filter out patterns that have less than 4 events with notes\n",
    "        #############################################################\n",
    "        # NOTE: more conditions could be added (i.e. kick on step 0, etc)\n",
    "        #############################################################\n",
    "        if patt_density > 4:\n",
    "            hv_lists_split.append(patt_fragment)\n",
    "\n",
    "  # output is a 16-step pattern\n",
    "    return hv_lists_split\n",
    "\n",
    "def find_unique_hv_lists(hv_lists_split):\n",
    "  # input a list of hv_lists and return the set of unique\n",
    "  unique_hv = list(set([tuple([tuple(step) for step in hv_list]) for hv_list in hv_lists]))\n",
    "  return unique_hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hv list flattening\n",
    "def flatten_hv_list(hv_list):\n",
    "  # input an hv list and output a flattened representation as a v_list\n",
    "\n",
    "  # list of instruments and categories\n",
    "  lows =  [35, 36, 41, 45, 47, 64, 66]\n",
    "  mids =  [37, 38, 39, 40, 43, 48, 50, 61, 62, 65, 68, 77]\n",
    "  his = [22, 26, 42, 44, 46, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 67, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81]\n",
    "\n",
    "  flat = np.zeros([len(hv_list),1])\n",
    "\n",
    "  # multiply velocities and categories\n",
    "  for i,step in enumerate(hv_list):\n",
    "    step_weight = 0\n",
    "    for onset in step:\n",
    "      if onset[0] in lows:\n",
    "        step_weight += onset[1]*3\n",
    "      elif onset[0] in mids:\n",
    "        step_weight += onset[1]*2\n",
    "      else:\n",
    "        step_weight += onset[1]*1\n",
    "    flat[i] = step_weight\n",
    "\n",
    "  flat = flat/max(flat)\n",
    "  return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MV_flattening(dualized):\n",
    "  flat = []\n",
    "  for step in dualized:\n",
    "    if len(step) == 2:\n",
    "      maximum = max([x[1] for x in step])\n",
    "    elif len(step) == 1:\n",
    "      maximum = step[0][1]\n",
    "    else:\n",
    "      maximum = 0\n",
    "    flat.append(maximum)\n",
    "  return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all patterns tested with four participants\n",
    "folder_path = '/Users/danielmartinezvillegas/Developer/master-ds/✨TDG/tdg_notebooks/dataset/midi_files/Repetitions/tested_with_four_participants'\n",
    "\n",
    "# List all files and subfolders in the folder\n",
    "folder_names_4p, file_names_4p = list_files_and_subfolders(folder_path)\n",
    "\n",
    "\n",
    "folders_4p = [] # make a container for a data structure of folder names\n",
    "originals_hv_4p = [] # save the original version in HV format\n",
    "originals_4p = [] # container for all original flattened patterns\n",
    "subject_4p = [] # make a container for all subject IDs\n",
    "repetition_4p = [] #contained for repetition ID\n",
    "flat_repetition_4p = [] #container for flat repetition\n",
    "ids_4p = [] # container for pattern IDs\n",
    "\n",
    "for subfolder in folder_names_4p:\n",
    "  #print (subfolder.replace(folder_path,\"\"))\n",
    "\n",
    "  # process original, there is always an \"original.mid\" in each folder\n",
    "  originals = midifile2hv_list(subfolder+\"/original.mid\", \"allinstruments\")\n",
    "  originals_flat = [flatten_hv_list(x).T for x in originals]\n",
    "  originals_flat = np.concatenate((originals_flat[0], originals_flat[1]))\n",
    "  originals_flat = np.ravel(originals_flat)\n",
    "  #print (\"original\", originals_flat)\n",
    "\n",
    "  # now process the repetitions done by subjects\n",
    "  s,files_in_subfolder = list_files_and_subfolders(subfolder)\n",
    "  for file_ in files_in_subfolder:\n",
    "    if file_ != subfolder+\"/original.mid\":\n",
    "      #extract participant name and repetition\n",
    "      participant = file_.replace(subfolder, \"\").split(\".\")[0].split(\"_\")[1]\n",
    "      repetition = file_.replace(subfolder, \"\").split(\".\")[0].split(\"_\")[3]\n",
    "      # flatten the drum pattern\n",
    "      hv_list = midifile2hv_list(file_, \"allinstruments\")\n",
    "      patt_id = int(subfolder.replace(folder_path,\"\").split(\" \")[0].replace(\"/[\",\"\"))\n",
    "      dual_flat = []\n",
    "      for l in hv_list:\n",
    "        dual_flat.append(MV_flattening(l))\n",
    "      dual_flat = np.array([item for sublist in dual_flat for item in sublist])\n",
    "      #print (participant, repetition, dual_flat)\n",
    "\n",
    "      # create data structures for analysis\n",
    "      folders_4p.append(subfolder)\n",
    "      originals_4p.append(originals_flat)\n",
    "      subject_4p.append(participant)\n",
    "      repetition_4p.append(repetition)\n",
    "      flat_repetition_4p.append(dual_flat)\n",
    "      originals_hv_4p.append(originals)\n",
    "      ids_4p.append(patt_id)\n",
    "data_4p = {\"folder\":folders_4p,\n",
    "           \"pattern id\": ids_4p,\n",
    "           \"original hv\": originals_hv_4p,\n",
    "           \"original flat\":originals_4p,\n",
    "           \"subject\": subject_4p,\n",
    "           \"repetition\": repetition_4p,\n",
    "           \"dualized flat\": flat_repetition_4p}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.split('/')[11] for x in folder_names_4p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe\n",
    "df_4p = pd.DataFrame(data_4p)\n",
    "print (\"----- SUMMARY -----\")\n",
    "print (\"The 'df_4p' dataframe has a\", df_4p.shape, \"shape\")\n",
    "print (\"There are\", df_4p.shape[0], \"interpretations made by subjects to\", len(df_4p['pattern id'].unique()), \"unique drum patterns\")\n",
    "print (\"As an example, the dataframe has the following columns and content in each column:\")\n",
    "for i,n in enumerate(df_4p.columns.tolist()):\n",
    "  #print ()\n",
    "  print (\"*\", n, \":\",df_4p.iloc[0,i])\n",
    "print ()\n",
    "print (\"Please, note that the patterns are 32 steps long, thus len(original hv) =\", len(df_4p.iloc[0,2]), \"bars\")\n",
    "print (\"This means that we have TWO one-bar patterns times each one of the\", len(df_4p['pattern id'].unique()), \"unique patterns (\",len(df_4p['pattern id'].unique()) * 2, \"patterns in total).\" )\n",
    "print (\"And also\", df_4p.shape[0]*2, \"one-bar interpretations by subjects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4p.sort_values(by='pattern id')['pattern id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4p[df_4p['pattern id'] == 1]['original hv'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4p['original flat'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4p['dualized flat'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = list(set((df_4p[\"pattern id\"].tolist())))\n",
    "unique_ids\n",
    "ttd_patterns = []\n",
    "ttd_repetitions = []\n",
    "count = 0\n",
    "\n",
    "for id in unique_ids:\n",
    "  # look for hvlists that belong to each ids\n",
    "  twobar_hv = df_4p.loc[df_4p[\"pattern id\"] == id, \"original hv\"].values[0]\n",
    "\n",
    "  # split each hv_list pattern into two 16 steps (one-bar) hv_list: A and B\n",
    "  # they are already split so just select 0 or 1\n",
    "  hv_A = twobar_hv[0]\n",
    "  hv_B = twobar_hv[1]\n",
    "  ttd_patterns.append(hv_A)\n",
    "  ttd_patterns.append(hv_B)\n",
    "\n",
    "  # group the repetitions for each pattern in another list\n",
    "  id_repetitions = df_4p.loc[df_4p[\"pattern id\"] == id, \"dualized flat\"]\n",
    "  repetitions_for_id_A = []\n",
    "  repetitions_for_id_B = []\n",
    "  for repetition in id_repetitions:\n",
    "    repA = repetition[:16]\n",
    "    repB = repetition[16:]\n",
    "    repetitions_for_id_A.append(repA)\n",
    "    repetitions_for_id_B.append(repB)\n",
    "\n",
    "  ttd_repetitions.append(repetitions_for_id_A)\n",
    "  ttd_repetitions.append(repetitions_for_id_B)\n",
    "\n",
    "data_ttd = {\"pattern\": ttd_patterns,\n",
    "            \"repetitions\": ttd_repetitions\n",
    "            }\n",
    "df_e1 = pd.DataFrame(data_ttd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e1['pattern'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e1['repetitions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e1['pattern'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I load the 16 patterns used in the tap to drum experiment\n",
    "# as well as the taps made by each one of the 37 valid subjects\n",
    "# that participated in the experiment\n",
    "\n",
    "# unfortunately drive and colab do not allow to fetch the CWD of this .ipynb\n",
    "# therefore I have to look for it IN MY OWN address\n",
    "# in order for this cell to work for you you'll have to use your address to\n",
    "# the folder where this \"PDG - TTD T2D to FWOD\" is located\n",
    "\n",
    "ipynb_folder = \"/Users/danielmartinezvillegas/Developer/master-ds/✨TDG/tdg_notebooks/\"\n",
    "\n",
    "# load the MIDI files that subjects listened to and convert them to hv_list format\n",
    "t2d_hv_lists = []\n",
    "midi_files = os.listdir(ipynb_folder+'tap to drum taps/midi patterns')\n",
    "\n",
    "for file_name in midi_files:\n",
    "  hv_list = midifile2hv_list(ipynb_folder+'tap to drum taps/midi patterns/'+file_name, \"all instruments\")\n",
    "  t2d_hv_lists.append(hv_list)\n",
    "\n",
    "with open(ipynb_folder+'tap to drum taps/valid_subjects_normalized.pkl', 'rb') as file:\n",
    "    t2d_taps = pickle.load(file)\n",
    "\n",
    "# the \"t2d_taps\" file contains 37 lists of lists. Each list belongs to a pattern\n",
    "# the sublists are the taps made by this subject to eachh of the 16 patterns\n",
    "\n",
    "\n",
    "print( \"here we have\", len(t2d_taps), \"lists each belonging to a subject. Each list has\", len(t2d_taps[0]), \"taps carried out to the\", len(t2d_taps[0]), \"patterns in the stimuli. We need to transpose this data structure so we get 16 lists each with 37 repetitions.\")\n",
    "\n",
    "# \"transpose\" the list of lists so that the lists are now patterns (16) and each sublist has 37 lists (subjects)ç\n",
    "\n",
    "transposed = list(map(list, zip(*t2d_taps)))\n",
    "\n",
    "print (\"After transposing, we have\", len(transposed), \"lists, each with \", len (transposed[0]), \"repetitions.\")\n",
    "\n",
    "df2_e1 = df_e1.copy()\n",
    "\n",
    "# now we can add the list of repetitions to the df_e1 dataframe\n",
    "for i, aaa in enumerate(transposed):\n",
    "  # New row to insert\n",
    "  new_row = {'pattern': t2d_hv_lists[i][0], 'repetitions': transposed[i]}\n",
    "  # Insert the new row at the end\n",
    "  df2_e1.loc[len(df2_e1)] = new_row\n",
    "\n",
    "df2_e1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2_e1[~(df2_e1['pattern'].isin(df_e1['pattern']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t2d_hv_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_taps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we export the patterns and find the positions offline.\n",
    "# we do this because this colab is way too long\n",
    "all_patts = df_e1['pattern'].tolist()\n",
    "\n",
    "# export as pickle file\n",
    "with open('e1_all_hvs.pkl', 'wb') as file:\n",
    "    pickle.dump(all_patts, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1_positions= [[0.25481138, 0.49981508],\n",
    "[0.2179403, 0.51954602],\n",
    "[0.27777847, 0.06701446],\n",
    "[0.71630662, 0.25267993],\n",
    "[0.43041888, 0.42072654],\n",
    "[0.40519822, 0.48152895],\n",
    "[0.83222983, 0.54226289],\n",
    "[0.92902444, 0.29793656],\n",
    "[0.31190826, 0.25127162],\n",
    "[0.37440468, 0.38623256],\n",
    "[0.35099213, 0.30081959],\n",
    "[0.42376579, 0.31108265],\n",
    "[0.34432537, 0.0798511, ],\n",
    "[0.3905479, 0.16702623],\n",
    "[0.54045633, 0.03551223],\n",
    "[0.37957815, 0.05036911],\n",
    "[0.33028617, 0.44996507],\n",
    "[0.385125, 0.35979486],\n",
    "[0.24655232, 0.40189878],\n",
    "[0.56584918, 0.22009101],\n",
    "[0.2012851, 0.32869522],\n",
    "[0.22260865, 0.38787051],\n",
    "[0.13664515, 0.37005278],\n",
    "[0.29416913, 0.38489171],\n",
    "[0.18195024, 0.1791192, ],\n",
    "[0.19270558, 0.20844821],\n",
    "[0.46405735, 0.27302673],\n",
    "[0.40850124, 0.28987157],\n",
    "[0.76677301, 0.33896449],\n",
    "[0.70891163, 0.59471925],\n",
    "[0.66651777, 0.63579998],\n",
    "[0.62783619, 0.43221262],\n",
    "[0.49769044, 0.24926781],\n",
    "[0.50178841, 0.40933657],\n",
    "[0.35819347, 0.48532311],\n",
    "[0.44685095, 0.40419707],\n",
    "[0.41132253, 0.43267381],\n",
    "[0.42649261, 0.5220558, ],\n",
    "[0.32750971, 0.17332979],\n",
    "[0.46761106, 0.48329648],\n",
    "[0.75550354, 0.47642278],\n",
    "[0.93384121, 0.32070003],\n",
    "[0.29318741, 0.1739194, ],\n",
    "[0.42579347, 0.17202725],\n",
    "[0.35563183, 0.55346241],\n",
    "[0.52196573, 0.39835123],\n",
    "[0.29508658, 0.2322916, ],\n",
    "[0.27331406, 0.25483046],\n",
    "[0. , 0.46440547],\n",
    "[0.2742823, 0.70285346],\n",
    "[0.11828873, 0.52326595],\n",
    "[1. , 0.52866413],\n",
    "[0.18289882, 0.69457992],\n",
    "[0.29797083, 0.69244345],\n",
    "[0.33766636, 0.70401435],\n",
    "[0.01289696, 0.56200238],\n",
    "[0.12932893, 0.73133141],\n",
    "[0.44021464, 0.9328381, ],\n",
    "[0.58169354, 0.88064595],\n",
    "[0.32888519, 0.70114832],\n",
    "[0.33066353, 0.62362863],\n",
    "[0.59185347, 0.88432219],\n",
    "[0.74505267, 0.16550339],\n",
    "[0.10049918, 0.29120842]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the positions to the dataframe\n",
    "\n",
    "df2_e1['position'] = e1_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_e1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
